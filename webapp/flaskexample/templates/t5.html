
<!DOCTYPE html>
<html>
    <head>
         <!-- Bootstrap core CSS -->
    <link href="../static/css/bootstrap.min.css" rel="stylesheet">
        <!-- Custom styles for this template -->
        <link href="../static/css/starter-template.css" rel="stylesheet">
    <title>Feature extraction, Dimensionality Reduction, PCA, ... threads</title>
    </head>
    <body>
    <div class="container">
      
        <h3>Check out some of these threads:</h3>
        </br>
<p><font size="3"><a href="../t5_a1"> Q1 </a></font>: !channel has anyone tried implementing a multiple polynomial regression model in python? I couldn’t find any examples that used both together </p><p><font size="3"><a href="../t5_a2"> Q2 </a></font>: Dear alumni, I am a health insight fellow at Boston. For the insight project, I am trying to create a predictive model from a dataset that consists of ~2000 features and 10k+ data points. I hope to end up with 10-20 most significant features. Does anyone know a robust way of doing feature selection? Any suggestion is greatly appreciated. :thinking_face:</p><p><font size="3"><a href="../t5_a3"> Q3 </a></font>: Hi all, questions around feature importance from Random Forest.
We are building an explanatory model to find out risk drivers of employee safety. We have chosen Random Forest classification to fit an injury binary factor with features around work hours, training, weather, etc.. Roughly speaking, I plan to report that the most important features reported from the Random Forest model are the drivers. In terms of libraries we have been using *randomForest*, *rfPermute* and *party* in R. I am not a statistician by training. I have read every discussion online and all library manuals, but still cannot get my head around these questions.

1. The 2 most commonly used metrics are *Gini impurity importance* and *permutation accuracy importance*.
	a. Which one is a better metrics in my use case?
	b. When the ranking from these 2 metrics disagree, what should I do?

2. rfPermute and party also report *p-value* for each feature,
	a. Should p-value play any role in selecting important features? Why do we want to know p-value in feature importance?
	b. What does it mean when a feature is important, but it’s p-value is high?</p><p><font size="3"><a href="../t5_a4"> Q4 </a></font>: @U3THNEALF Have you tried jython? or jpype? that way you can at least run everthing in your python app. Jython tries to replicate java in python, while jpype also the python vm to to the java vm.</p><p><font size="3"><a href="../t5_a5"> Q5 </a></font>: Hi All, 
Need some insights from clustering experts here !
I am trying to find clusters in a dataset that has latitude-longitude as two of the features - I found that DBscan works well for lat-lon data - but does it work for lat_lon + other features (mostly categorical)?  Or should I try something else?
Thanks in advance for any pointers!</p><p><font size="3"><a href="../t5_a6"> Q6 </a></font>: ok, so i've got two _categorical_ features which i suspect are highly related or even overlapping, and i'm trying to figure out how to test that.  i'm imagining a 2D histogram with one feature on each axis and the color encoding the count of inputs in each bin.  is there an easy way to do this without encoding the categorical variables into numerical ones?  my google-fu is failing me...</p><p><font size="3"><a href="../t5_a7"> Q7 </a></font>: Good morning!  Does anyone have experience or a good understanding of what to do when you have highly redundant data samples with different target values?   I'm working on a consulting project with a PR firm to predict whether a journalist will click on an email heading containing a media pitch.  The data set consists of some emails that are only sent out once or twice while others are sent out more than a hundred times.  Any advice is welcome!</p><p><font size="3"><a href="../t5_a8"> Q8 </a></font>: Hi folks, is there a way to know in which way the feature is important in random forest? Such as positively correlated or negatively correlated to the predicted value? </p><p><font size="3"><a href="../t5_a9"> Q9 </a></font>: Linear regression question:
If i have both categorical and continuous features (&gt;0) what is the best way to preprocess the data? My continuous features have wide ranging scales - some are in [0,10], others are in [0,100000]. I can rescale the continuous ones and one-hot encode the categorical ones, but is that the right thing to do? Seems this scheme would favor the categorical variables. </p><p><font size="3"><a href="../t5_a10"> Q10 </a></font>: Hey Insight! I mostly do model building in the p &gt;&gt; N space, where feature selection is important, but really I work in the p &gt;&gt;&gt; N space, where I have very few samples (think less than 10 with hundreds of features, in some cases). One of the biggest issues I have with the pipelines I construct is validation— if I choose a sane and simple model, I am virtually guaranteed to get 100% training accuracy (which is likely overfitting). Cross validation is not really an option with such few samples. 

I am curious if any yinz have come up with other ways to validate such models in this corner of ML space. The ones I have used so far: first and foremost, domain-specific sanity checks (in microbial ecology, in my case), and also randomly initializing the models a few times and looking at the variance in the feature importances across those initializations (the hope being that if the model is truly doing well, there shouldn’t be high variance in the feature importance of the most important features). Any other tips anyone has got?</p><p><font size="3"><a href="../t5_a11"> Q11 </a></font>: Random feature engineering question! Would you consider survey answers to questions like on a scale of 1-10 categorical or numeric?</p>

        </br></br></br>
        
        <center>
            <font size="3" color=#CDC9C9>© 2018 <a href="mailto:duccio.pappadopulo@gmail.com?Subject=You%20screw%20up" target="_top">Duccio Pappadopulo</a> </font>
        </center>
        
        </body>
        
         </div>
    </html>
