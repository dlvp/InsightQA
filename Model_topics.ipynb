{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs two topic modelling algorithms to model topics appearing in Data Science Stack Exchange. </br>\n",
    "The first algorithm is LDA with 15 topics, the second algorithm is NMF with 15 topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "- Read Data Science Stack Exchange database\n",
    "- Run LDA and show most probable words in each of the 15 topics\n",
    "- Run NMF and show most probable words in each of the 15 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning!\n",
    "<b> Some of the files which are necessary to run this notebook are not contained on this repository! </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import pylab\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#os stuff\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rcParams['font.size'] = 14\n",
    "matplotlib.rcParams['legend.fontsize'] = 'medium'\n",
    "matplotlib.rcParams['figure.titlesize'] = 'small'\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "pylab.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_link(s):\n",
    "    return re.sub(r'<http\\S+>', ' <link> ', s)\n",
    "\n",
    "def re_emoji(s):\n",
    "    return re.sub(r':\\S+:', ' <emoji> ', s)\n",
    "\n",
    "def re_html(s):\n",
    "    return re.sub(r'<\\S+>', ' ', s)\n",
    "\n",
    "def re_html_plus(s):\n",
    "    return re.sub(r'<[^>]+>', ' ', s)\n",
    "\n",
    "def re_html_plus(s):\n",
    "    return re.sub(r'<.+?>', ' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DS SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SE=pd.read_json('DSSE_Posts.json')\n",
    "data_SE=df_SE.copy() #contains posts titles, bodies, answers and dates, BUT NO TAGS\n",
    "\n",
    "df_SE_tags=pd.read_json('DSSE_History.json')\n",
    "data_SE_tags=df_SE_tags.copy() #contains posts titles, dates AND TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add tags to df_SE, matching on the date the post is first published\n",
    "df_tags = pd.merge(data_SE_tags[data_SE_tags.time.isnull()], data_SE_tags[-data_SE_tags.time.isnull()], on=['ID','ID'])\n",
    "df_tags = df_tags.drop('time_x', 1)\n",
    "df_tags.rename(columns={'text_x': 'tags', 'text_y': 'text','time_y': 'time'}, inplace=True)\n",
    "times_to_tags = dict(zip(df_tags['time'].values,df_tags['tags'].values))\n",
    "data_SE['tags'] = data_SE.time.map(times_to_tags)\n",
    "data_SE = data_SE.drop('time', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I do simple machine learning without hard-coding behavior?</td>\n",
       "      <td>title</td>\n",
       "      <td>&lt;machine-learning&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                                                text  \\\n",
       "0  5   How can I do simple machine learning without hard-coding behavior?   \n",
       "\n",
       "    type                tags  \n",
       "0  title  <machine-learning>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_SE.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For processing I can join the text of posts with the same index\n",
    "df_ID_tags = data_SE[data_SE.tags.notnull()][['ID','tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_SE['text'] = data_SE['text'].apply(lambda x : ' '+ x )\n",
    "s_sum_text = data_SE[['ID','text']].groupby(['ID']).text.sum()\n",
    "df_sum_text = pd.DataFrame({'ID':s_sum_text.index, 'text':s_sum_text.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I do simple machine learning without hard-coding behavior? &lt;p&gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple \"Hello World\" example - how can I avoid hard-coding behavior?&lt;/p&gt;\\n\\n&lt;p&gt;For example, if I wanted to \"teach\" a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&lt;/p&gt;\\n\\n&lt;p&gt;Obviously, randomly generating code would be impractical, so how could I do this?&lt;/p&gt;\\n &lt;p&gt;Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.&lt;/p&gt;\\n\\n&lt;p&gt;With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are \"fixed\" and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&lt;/p&gt;\\n\\n&lt;p&gt;You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&lt;/p&gt;\\n\\n&lt;p&gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&lt;/p&gt;\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  \\\n",
       "0  5    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "0   How can I do simple machine learning without hard-coding behavior? <p>I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple \"Hello World\" example - how can I avoid hard-coding behavior?</p>\\n\\n<p>For example, if I wanted to \"teach\" a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.</p>\\n\\n<p>Obviously, randomly generating code would be impractical, so how could I do this?</p>\\n <p>Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.</p>\\n\\n<p>With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are \"fixed\" and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.</p>\\n\\n<p>You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.</p>\\n\\n<p>In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.</p>\\n  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_text.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_tags = pd.merge(df_ID_tags,df_sum_text, on=['ID','ID'])\n",
    "df_text_tags['text'] = df_text_tags['text'].apply(lambda x : re_html_plus(x) )\n",
    "df_text_tags['length'] = df_text_tags['text'].apply(lambda x : len(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the most used tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_val =df_text_tags.tags.values\n",
    "alltags = [tag for stringtags in tags_val for tag in re.findall('<(.*?)>', stringtags) ]\n",
    "alltags_table = [re.findall('<(.*?)>', _) for _ in tags_val]\n",
    "n_tags = [len(_) for _ in alltags_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tags):\n",
    "    re.findall('<(.*?)>', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_count = Counter(alltags)\n",
    "dict_tag_count  = dict(tag_count)\n",
    "tag_count_sorted = sorted(dict_tag_count.items(), key=lambda x: x[1], reverse = True)\n",
    "sub_tags=dict(tag_count_sorted[:20]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'clustering',\n",
       " u'nlp',\n",
       " u'statistics',\n",
       " u'feature-selection',\n",
       " u'machine-learning',\n",
       " u'classification',\n",
       " u'keras',\n",
       " u'python',\n",
       " u'data-mining',\n",
       " u'scikit-learn',\n",
       " u'bigdata',\n",
       " u'dataset',\n",
       " u'deep-learning',\n",
       " u'r',\n",
       " u'neural-network',\n",
       " u'text-mining',\n",
       " u'time-series',\n",
       " u'tensorflow',\n",
       " u'predictive-modeling',\n",
       " u'regression']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 20 most used tags in DS_SE\n",
    "sub_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA for Data Science Stack Exchange (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts  = df_text_tags.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts_train, texts_test = train_test_split(texts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import preprocess_string,strip_tags,strip_punctuation, remove_stopwords,strip_numeric\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['q','w','e','t','y'\\\n",
    "                   ,'u', 'i', 'o', 'p', 'l', 'j', 'h', 'g', 'f', 'd', 's', 'a', \\\n",
    "                   'z', 'x', 'c', 'v', 'b', 'n', 'm'])\n",
    "\n",
    "# stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(text): #,stem): \n",
    "    #tokens = remove_special(text)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\n",
    "    tokens = preprocess_string(text, CUSTOM_FILTERS)\n",
    "    tokens = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', ' '.join(tokens)).split()\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize corpus and create dictionary\n",
    "tokenized_texts = [parser(_) for _ in texts]\n",
    "dict_DSSE = corpora.Dictionary(tokenized_texts)\n",
    "dict_DSSE.filter_extremes(no_below=10, no_above=0.9)\n",
    "corpus = [dict_DSSE.doc2bow(_) for _ in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 7482\n"
     ]
    }
   ],
   "source": [
    "#dict_DSSE.save('dict_DSSE_10plus.gensim')\n",
    "#corpora.Dictionary.load('dict_stemmed_DSSE.gensim') #To reload dictionary\n",
    "print('Dictionary size: {}').format(len(dict_DSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(_) for _ in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, num_topics=15, id2word=dict_DSSE, chunksize=200, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics visualization\n",
    "p = pyLDAvis.gensim.prepare(lda, corpus, dict_DSSE, sort_topics=False)\n",
    "pyLDAvis.save_html(p, 'lda_test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20);\n",
    "        words = [_[0] for _ in words];\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the 20 most significant words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "      <th>Topic # 11</th>\n",
       "      <th>Topic # 12</th>\n",
       "      <th>Topic # 13</th>\n",
       "      <th>Topic # 14</th>\n",
       "      <th>Topic # 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>python</td>\n",
       "      <td>https</td>\n",
       "      <td>model</td>\n",
       "      <td>words</td>\n",
       "      <td>state</td>\n",
       "      <td>features</td>\n",
       "      <td>input</td>\n",
       "      <td>data</td>\n",
       "      <td>class</td>\n",
       "      <td>image</td>\n",
       "      <td>function</td>\n",
       "      <td>learning</td>\n",
       "      <td>gt</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time</td>\n",
       "      <td>r</td>\n",
       "      <td>com</td>\n",
       "      <td>train</td>\n",
       "      <td>text</td>\n",
       "      <td>amp</td>\n",
       "      <td>feature</td>\n",
       "      <td>network</td>\n",
       "      <td>would</td>\n",
       "      <td>classification</td>\n",
       "      <td>images</td>\n",
       "      <td>k</td>\n",
       "      <td>data</td>\n",
       "      <td>np</td>\n",
       "      <td>spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>values</td>\n",
       "      <td>using</td>\n",
       "      <td>generator</td>\n",
       "      <td>test</td>\n",
       "      <td>word</td>\n",
       "      <td>learning</td>\n",
       "      <td>model</td>\n",
       "      <td>output</td>\n",
       "      <td>one</td>\n",
       "      <td>classes</td>\n",
       "      <td>cnn</td>\n",
       "      <td>sum</td>\n",
       "      <td>machine</td>\n",
       "      <td>import</td>\n",
       "      <td>files</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>series</td>\n",
       "      <td>code</td>\n",
       "      <td>paper</td>\n",
       "      <td>data</td>\n",
       "      <td>vector</td>\n",
       "      <td>probability</td>\n",
       "      <td>regression</td>\n",
       "      <td>tf</td>\n",
       "      <td>use</td>\n",
       "      <td>label</td>\n",
       "      <td>img</td>\n",
       "      <td>gradient</td>\n",
       "      <td>deep</td>\n",
       "      <td>lt</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>value</td>\n",
       "      <td>use</td>\n",
       "      <td>http</td>\n",
       "      <td>training</td>\n",
       "      <td>embedding</td>\n",
       "      <td>action</td>\n",
       "      <td>linear</td>\n",
       "      <td>layer</td>\n",
       "      <td>like</td>\n",
       "      <td>cluster</td>\n",
       "      <td>width</td>\n",
       "      <td>frac</td>\n",
       "      <td>science</td>\n",
       "      <td>self</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>variables</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>org</td>\n",
       "      <td>set</td>\n",
       "      <td>similarity</td>\n",
       "      <td>distribution</td>\n",
       "      <td>tree</td>\n",
       "      <td>batch</td>\n",
       "      <td>problem</td>\n",
       "      <td>labels</td>\n",
       "      <td>feature</td>\n",
       "      <td>mean</td>\n",
       "      <td>interest</td>\n",
       "      <td>print</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>variable</td>\n",
       "      <td>graph</td>\n",
       "      <td>html</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>using</td>\n",
       "      <td>reward</td>\n",
       "      <td>decision</td>\n",
       "      <td>keras</td>\n",
       "      <td>could</td>\n",
       "      <td>classifier</td>\n",
       "      <td>object</td>\n",
       "      <td>value</td>\n",
       "      <td>learn</td>\n",
       "      <td>df</td>\n",
       "      <td>lib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>categorical</td>\n",
       "      <td>data</td>\n",
       "      <td>google</td>\n",
       "      <td>validation</td>\n",
       "      <td>vectors</td>\n",
       "      <td>beta</td>\n",
       "      <td>models</td>\n",
       "      <td>size</td>\n",
       "      <td>need</td>\n",
       "      <td>clustering</td>\n",
       "      <td>features</td>\n",
       "      <td>partial</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>plt</td>\n",
       "      <td>site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one</td>\n",
       "      <td>like</td>\n",
       "      <td>github</td>\n",
       "      <td>dataset</td>\n",
       "      <td>sentence</td>\n",
       "      <td>states</td>\n",
       "      <td>using</td>\n",
       "      <td>neural</td>\n",
       "      <td>different</td>\n",
       "      <td>k</td>\n",
       "      <td>red</td>\n",
       "      <td>matrix</td>\n",
       "      <td>course</td>\n",
       "      <td>array</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>column</td>\n",
       "      <td>want</td>\n",
       "      <td>www</td>\n",
       "      <td>loss</td>\n",
       "      <td>document</td>\n",
       "      <td>value</td>\n",
       "      <td>use</td>\n",
       "      <td>activation</td>\n",
       "      <td>want</td>\n",
       "      <td>score</td>\n",
       "      <td>size</td>\n",
       "      <td>cost</td>\n",
       "      <td>company</td>\n",
       "      <td>return</td>\n",
       "      <td>hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>columns</td>\n",
       "      <td>plot</td>\n",
       "      <td>dataset</td>\n",
       "      <td>cross</td>\n",
       "      <td>documents</td>\n",
       "      <td>agent</td>\n",
       "      <td>random</td>\n",
       "      <td>layers</td>\n",
       "      <td>also</td>\n",
       "      <td>data</td>\n",
       "      <td>convolutional</td>\n",
       "      <td>error</td>\n",
       "      <td>team</td>\n",
       "      <td>true</td>\n",
       "      <td>apache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>date</td>\n",
       "      <td>package</td>\n",
       "      <td>adam</td>\n",
       "      <td>predict</td>\n",
       "      <td>use</td>\n",
       "      <td>current</td>\n",
       "      <td>svm</td>\n",
       "      <td>loss</td>\n",
       "      <td>time</td>\n",
       "      <td>clusters</td>\n",
       "      <td>height</td>\n",
       "      <td>values</td>\n",
       "      <td>ml</td>\n",
       "      <td>index</td>\n",
       "      <td>packages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dataset</td>\n",
       "      <td>batches</td>\n",
       "      <td>link</td>\n",
       "      <td>using</td>\n",
       "      <td>list</td>\n",
       "      <td>next</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>weights</td>\n",
       "      <td>example</td>\n",
       "      <td>probability</td>\n",
       "      <td>filter</td>\n",
       "      <td>r</td>\n",
       "      <td>analysis</td>\n",
       "      <td>data</td>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>predict</td>\n",
       "      <td>library</td>\n",
       "      <td>pdf</td>\n",
       "      <td>dense</td>\n",
       "      <td>model</td>\n",
       "      <td>stock</td>\n",
       "      <td>trees</td>\n",
       "      <td>lstm</td>\n",
       "      <td>question</td>\n",
       "      <td>recall</td>\n",
       "      <td>trained</td>\n",
       "      <td>theta</td>\n",
       "      <td>models</td>\n",
       "      <td>code</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>want</td>\n",
       "      <td>orange</td>\n",
       "      <td>page</td>\n",
       "      <td>error</td>\n",
       "      <td>want</td>\n",
       "      <td>actions</td>\n",
       "      <td>variables</td>\n",
       "      <td>add</td>\n",
       "      <td>way</td>\n",
       "      <td>use</td>\n",
       "      <td>color</td>\n",
       "      <td>right</td>\n",
       "      <td>book</td>\n",
       "      <td>def</td>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>like</td>\n",
       "      <td>pandas</td>\n",
       "      <td>find</td>\n",
       "      <td>split</td>\n",
       "      <td>extract</td>\n",
       "      <td>game</td>\n",
       "      <td>pca</td>\n",
       "      <td>shape</td>\n",
       "      <td>may</td>\n",
       "      <td>precision</td>\n",
       "      <td>cat</td>\n",
       "      <td>weights</td>\n",
       "      <td>etc</td>\n",
       "      <td>none</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>missing</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>found</td>\n",
       "      <td>predictions</td>\n",
       "      <td>sentences</td>\n",
       "      <td>sample</td>\n",
       "      <td>classification</td>\n",
       "      <td>relu</td>\n",
       "      <td>good</td>\n",
       "      <td>positive</td>\n",
       "      <td>channels</td>\n",
       "      <td>vector</td>\n",
       "      <td>project</td>\n",
       "      <td>list</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hot</td>\n",
       "      <td>way</td>\n",
       "      <td>papers</td>\n",
       "      <td>fit</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>given</td>\n",
       "      <td>logistic</td>\n",
       "      <td>inputs</td>\n",
       "      <td>based</td>\n",
       "      <td>dataset</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>log</td>\n",
       "      <td>research</td>\n",
       "      <td>shape</td>\n",
       "      <td>gpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>numeric</td>\n",
       "      <td>help</td>\n",
       "      <td>article</td>\n",
       "      <td>epoch</td>\n",
       "      <td>length</td>\n",
       "      <td>gamma</td>\n",
       "      <td>selection</td>\n",
       "      <td>nn</td>\n",
       "      <td>using</td>\n",
       "      <td>means</td>\n",
       "      <td>pre</td>\n",
       "      <td>end</td>\n",
       "      <td>like</td>\n",
       "      <td>get</td>\n",
       "      <td>org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>day</td>\n",
       "      <td>im</td>\n",
       "      <td>source</td>\n",
       "      <td>size</td>\n",
       "      <td>example</td>\n",
       "      <td>model</td>\n",
       "      <td>learn</td>\n",
       "      <td>dropout</td>\n",
       "      <td>might</td>\n",
       "      <td>binary</td>\n",
       "      <td>cv2</td>\n",
       "      <td>left</td>\n",
       "      <td>neural</td>\n",
       "      <td>random</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic # 01  Topic # 02 Topic # 03   Topic # 04  Topic # 05    Topic # 06  \\\n",
       "0   data         python      https      model        words       state          \n",
       "1   time         r           com        train        text        amp            \n",
       "2   values       using       generator  test         word        learning       \n",
       "3   series       code        paper      data         vector      probability    \n",
       "4   value        use         http       training     embedding   action         \n",
       "5   variables    tensorflow  org        set          similarity  distribution   \n",
       "6   variable     graph       html       accuracy     using       reward         \n",
       "7   categorical  data        google     validation   vectors     beta           \n",
       "8   one          like        github     dataset      sentence    states         \n",
       "9   column       want        www        loss         document    value          \n",
       "10  columns      plot        dataset    cross        documents   agent          \n",
       "11  date         package     adam       predict      use         current        \n",
       "12  dataset      batches     link       using        list        next           \n",
       "13  predict      library     pdf        dense        model       stock          \n",
       "14  want         orange      page       error        want        actions        \n",
       "15  like         pandas      find       split        extract     game           \n",
       "16  missing      pytorch     found      predictions  sentences   sample         \n",
       "17  hot          way         papers     fit          word2vec    given          \n",
       "18  numeric      help        article    epoch        length      gamma          \n",
       "19  day          im          source     size         example     model          \n",
       "\n",
       "        Topic # 07  Topic # 08 Topic # 09      Topic # 10     Topic # 11  \\\n",
       "0   features        input       data       class           image           \n",
       "1   feature         network     would      classification  images          \n",
       "2   model           output      one        classes         cnn             \n",
       "3   regression      tf          use        label           img             \n",
       "4   linear          layer       like       cluster         width           \n",
       "5   tree            batch       problem    labels          feature         \n",
       "6   decision        keras       could      classifier      object          \n",
       "7   models          size        need       clustering      features        \n",
       "8   using           neural      different  k               red             \n",
       "9   use             activation  want       score           size            \n",
       "10  random          layers      also       data            convolutional   \n",
       "11  svm             loss        time       clusters        height          \n",
       "12  algorithm       weights     example    probability     filter          \n",
       "13  trees           lstm        question   recall          trained         \n",
       "14  variables       add         way        use             color           \n",
       "15  pca             shape       may        precision       cat             \n",
       "16  classification  relu        good       positive        channels        \n",
       "17  logistic        inputs      based      dataset         embeddings      \n",
       "18  selection       nn          using      means           pre             \n",
       "19  learn           dropout     might      binary          cv2             \n",
       "\n",
       "   Topic # 12  Topic # 13 Topic # 14 Topic # 15  \n",
       "0   function   learning    gt         file       \n",
       "1   k          data        np         spark      \n",
       "2   sum        machine     import     files      \n",
       "3   gradient   deep        lt         py         \n",
       "4   frac       science     self       memory     \n",
       "5   mean       interest    print      data       \n",
       "6   value      learn       df         lib        \n",
       "7   partial    algorithms  plt        site       \n",
       "8   matrix     course      array      line       \n",
       "9   cost       company     return     hadoop     \n",
       "10  error      team        true       apache     \n",
       "11  values     ml          index      packages   \n",
       "12  r          analysis    data       csv        \n",
       "13  theta      models      code       run        \n",
       "14  right      book        def        path       \n",
       "15  weights    etc         none       read       \n",
       "16  vector     project     list       python     \n",
       "17  log        research    shape      gpu        \n",
       "18  end        like        get        org        \n",
       "19  left       neural      random     name       "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(lda, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts  = df_text_tags.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(text): #,stem): \n",
    "    #tokens = remove_special(text)\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\n",
    "    tokens = preprocess_string(text, CUSTOM_FILTERS)\n",
    "    tokens = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', ' '.join(tokens)).split()\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    #stemming\n",
    "    #tokens = [stemmer.lemmatize(x) for x in tokens]\n",
    "    #tokens = [re.sub(\"\\'\", \"\",re.sub('\\S*@\\S*\\s?', '', word.lower())) for word in nltk.word_tokenize(text)]\n",
    "    #tokens = [x for x in tokens if not x in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_NMF = [' '.join(parser( _ )) for _ in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer='word',max_df=0.9, min_df=10,max_features=5000)#, max_features=5000)\n",
    "vect_model = vect.fit(texts_NMF)\n",
    "x_counts = vect_model.transform(texts_NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf_model = transformer.fit(x_counts)\n",
    "x_tfidf = tfidf_model.transform(x_counts)\n",
    "x_tfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=15, random_state=None, shuffle=False,\n",
       "  solver='cd', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtain a NMF model.\n",
    "num_topics=15\n",
    "model_NMF = NMF(n_components=num_topics, init='nndsvd');\n",
    "#fit the model\n",
    "model_NMF.fit(x_tfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vect.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the most significant words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "      <th>Topic # 11</th>\n",
       "      <th>Topic # 12</th>\n",
       "      <th>Topic # 13</th>\n",
       "      <th>Topic # 14</th>\n",
       "      <th>Topic # 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>gt</td>\n",
       "      <td>model</td>\n",
       "      <td>test</td>\n",
       "      <td>regression</td>\n",
       "      <td>df</td>\n",
       "      <td>words</td>\n",
       "      <td>features</td>\n",
       "      <td>time</td>\n",
       "      <td>na</td>\n",
       "      <td>clustering</td>\n",
       "      <td>network</td>\n",
       "      <td>data</td>\n",
       "      <td>partial</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine</td>\n",
       "      <td>lt</td>\n",
       "      <td>tf</td>\n",
       "      <td>train</td>\n",
       "      <td>tree</td>\n",
       "      <td>pandas</td>\n",
       "      <td>word</td>\n",
       "      <td>feature</td>\n",
       "      <td>series</td>\n",
       "      <td>lt</td>\n",
       "      <td>cluster</td>\n",
       "      <td>neural</td>\n",
       "      <td>science</td>\n",
       "      <td>frac</td>\n",
       "      <td>images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python</td>\n",
       "      <td>java</td>\n",
       "      <td>keras</td>\n",
       "      <td>validation</td>\n",
       "      <td>variables</td>\n",
       "      <td>dataframe</td>\n",
       "      <td>text</td>\n",
       "      <td>selection</td>\n",
       "      <td>lstm</td>\n",
       "      <td>missing</td>\n",
       "      <td>distance</td>\n",
       "      <td>layer</td>\n",
       "      <td>set</td>\n",
       "      <td>theta</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spark</td>\n",
       "      <td>label</td>\n",
       "      <td>loss</td>\n",
       "      <td>training</td>\n",
       "      <td>variable</td>\n",
       "      <td>pd</td>\n",
       "      <td>word2vec</td>\n",
       "      <td>pca</td>\n",
       "      <td>day</td>\n",
       "      <td>nan</td>\n",
       "      <td>clusters</td>\n",
       "      <td>output</td>\n",
       "      <td>orange</td>\n",
       "      <td>amp</td>\n",
       "      <td>cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>string</td>\n",
       "      <td>batch</td>\n",
       "      <td>model</td>\n",
       "      <td>decision</td>\n",
       "      <td>np</td>\n",
       "      <td>document</td>\n",
       "      <td>importance</td>\n",
       "      <td>predict</td>\n",
       "      <td>inferred</td>\n",
       "      <td>similarity</td>\n",
       "      <td>input</td>\n",
       "      <td>analysis</td>\n",
       "      <td>function</td>\n",
       "      <td>classes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user</td>\n",
       "      <td>list</td>\n",
       "      <td>shape</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>linear</td>\n",
       "      <td>columns</td>\n",
       "      <td>documents</td>\n",
       "      <td>extraction</td>\n",
       "      <td>rnn</td>\n",
       "      <td>id</td>\n",
       "      <td>matrix</td>\n",
       "      <td>layers</td>\n",
       "      <td>big</td>\n",
       "      <td>sigma</td>\n",
       "      <td>dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>xml</td>\n",
       "      <td>input</td>\n",
       "      <td>set</td>\n",
       "      <td>categorical</td>\n",
       "      <td>csv</td>\n",
       "      <td>sentence</td>\n",
       "      <td>correlation</td>\n",
       "      <td>prediction</td>\n",
       "      <td>values</td>\n",
       "      <td>means</td>\n",
       "      <td>weights</td>\n",
       "      <td>like</td>\n",
       "      <td>sum</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>frame</td>\n",
       "      <td>activation</td>\n",
       "      <td>cross</td>\n",
       "      <td>logistic</td>\n",
       "      <td>id</td>\n",
       "      <td>vectors</td>\n",
       "      <td>categorical</td>\n",
       "      <td>days</td>\n",
       "      <td>error</td>\n",
       "      <td>cosine</td>\n",
       "      <td>networks</td>\n",
       "      <td>file</td>\n",
       "      <td>cost</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>would</td>\n",
       "      <td>row</td>\n",
       "      <td>dense</td>\n",
       "      <td>score</td>\n",
       "      <td>model</td>\n",
       "      <td>column</td>\n",
       "      <td>vector</td>\n",
       "      <td>vector</td>\n",
       "      <td>date</td>\n",
       "      <td>frame</td>\n",
       "      <td>points</td>\n",
       "      <td>hidden</td>\n",
       "      <td>new</td>\n",
       "      <td>error</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https</td>\n",
       "      <td>function</td>\n",
       "      <td>size</td>\n",
       "      <td>class</td>\n",
       "      <td>trees</td>\n",
       "      <td>import</td>\n",
       "      <td>topic</td>\n",
       "      <td>importances</td>\n",
       "      <td>sequence</td>\n",
       "      <td>auto</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>convolutional</td>\n",
       "      <td>would</td>\n",
       "      <td>gradient</td>\n",
       "      <td>detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>like</td>\n",
       "      <td>td</td>\n",
       "      <td>add</td>\n",
       "      <td>fold</td>\n",
       "      <td>values</td>\n",
       "      <td>nan</td>\n",
       "      <td>embedding</td>\n",
       "      <td>dataset</td>\n",
       "      <td>model</td>\n",
       "      <td>jan</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>neurons</td>\n",
       "      <td>missing</td>\n",
       "      <td>hat</td>\n",
       "      <td>labels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>com</td>\n",
       "      <td>library</td>\n",
       "      <td>tensorflow</td>\n",
       "      <td>dataset</td>\n",
       "      <td>class</td>\n",
       "      <td>index</td>\n",
       "      <td>corpus</td>\n",
       "      <td>classifier</td>\n",
       "      <td>event</td>\n",
       "      <td>feb</td>\n",
       "      <td>two</td>\n",
       "      <td>net</td>\n",
       "      <td>pca</td>\n",
       "      <td>mathbf</td>\n",
       "      <td>imagenet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>using</td>\n",
       "      <td>order</td>\n",
       "      <td>relu</td>\n",
       "      <td>split</td>\n",
       "      <td>classification</td>\n",
       "      <td>date</td>\n",
       "      <td>lda</td>\n",
       "      <td>one</td>\n",
       "      <td>month</td>\n",
       "      <td>mar</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>function</td>\n",
       "      <td>table</td>\n",
       "      <td>log</td>\n",
       "      <td>convolutional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>error</td>\n",
       "      <td>self</td>\n",
       "      <td>error</td>\n",
       "      <td>random</td>\n",
       "      <td>file</td>\n",
       "      <td>topics</td>\n",
       "      <td>important</td>\n",
       "      <td>arima</td>\n",
       "      <td>function</td>\n",
       "      <td>number</td>\n",
       "      <td>number</td>\n",
       "      <td>different</td>\n",
       "      <td>loss</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>find</td>\n",
       "      <td>get</td>\n",
       "      <td>np</td>\n",
       "      <td>testing</td>\n",
       "      <td>one</td>\n",
       "      <td>row</td>\n",
       "      <td>embeddings</td>\n",
       "      <td>correlated</td>\n",
       "      <td>anomaly</td>\n",
       "      <td>matrix</td>\n",
       "      <td>method</td>\n",
       "      <td>training</td>\n",
       "      <td>lt</td>\n",
       "      <td>mu</td>\n",
       "      <td>trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>system</td>\n",
       "      <td>net</td>\n",
       "      <td>lstm</td>\n",
       "      <td>classifier</td>\n",
       "      <td>value</td>\n",
       "      <td>values</td>\n",
       "      <td>similarity</td>\n",
       "      <td>different</td>\n",
       "      <td>one</td>\n",
       "      <td>correlation</td>\n",
       "      <td>use</td>\n",
       "      <td>activation</td>\n",
       "      <td>good</td>\n",
       "      <td>alpha</td>\n",
       "      <td>bounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>learn</td>\n",
       "      <td>class</td>\n",
       "      <td>output</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>continuous</td>\n",
       "      <td>plt</td>\n",
       "      <td>sentences</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>forecast</td>\n",
       "      <td>numeric</td>\n",
       "      <td>similar</td>\n",
       "      <td>convolution</td>\n",
       "      <td>mining</td>\n",
       "      <td>equation</td>\n",
       "      <td>inception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>models</td>\n",
       "      <td>name</td>\n",
       "      <td>import</td>\n",
       "      <td>cv</td>\n",
       "      <td>forest</td>\n",
       "      <td>name</td>\n",
       "      <td>context</td>\n",
       "      <td>space</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>mean</td>\n",
       "      <td>metric</td>\n",
       "      <td>neuron</td>\n",
       "      <td>sets</td>\n",
       "      <td>delta</td>\n",
       "      <td>resolution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>package</td>\n",
       "      <td>command</td>\n",
       "      <td>layer</td>\n",
       "      <td>predict</td>\n",
       "      <td>predict</td>\n",
       "      <td>print</td>\n",
       "      <td>doc2vec</td>\n",
       "      <td>engineering</td>\n",
       "      <td>would</td>\n",
       "      <td>factor</td>\n",
       "      <td>find</td>\n",
       "      <td>cnn</td>\n",
       "      <td>want</td>\n",
       "      <td>bmatrix</td>\n",
       "      <td>recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>length</td>\n",
       "      <td>train</td>\n",
       "      <td>performance</td>\n",
       "      <td>binary</td>\n",
       "      <td>plot</td>\n",
       "      <td>idf</td>\n",
       "      <td>use</td>\n",
       "      <td>signal</td>\n",
       "      <td>rating</td>\n",
       "      <td>graph</td>\n",
       "      <td>inputs</td>\n",
       "      <td>training</td>\n",
       "      <td>derivative</td>\n",
       "      <td>pixel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic # 01 Topic # 02  Topic # 03   Topic # 04      Topic # 05 Topic # 06  \\\n",
       "0   learning    gt         model       test         regression      df          \n",
       "1   machine     lt         tf          train        tree            pandas      \n",
       "2   python      java       keras       validation   variables       dataframe   \n",
       "3   spark       label      loss        training     variable        pd          \n",
       "4   deep        string     batch       model        decision        np          \n",
       "5   user        list       shape       accuracy     linear          columns     \n",
       "6   algorithm   xml        input       set          categorical     csv         \n",
       "7   use         frame      activation  cross        logistic        id          \n",
       "8   would       row        dense       score        model           column      \n",
       "9   https       function   size        class        trees           import      \n",
       "10  like        td         add         fold         values          nan         \n",
       "11  com         library    tensorflow  dataset      class           index       \n",
       "12  using       order      relu        split        classification  date        \n",
       "13  algorithms  error      self        error        random          file        \n",
       "14  find        get        np          testing      one             row         \n",
       "15  system      net        lstm        classifier   value           values      \n",
       "16  learn       class      output      sklearn      continuous      plt         \n",
       "17  models      name       import      cv           forest          name        \n",
       "18  package     command    layer       predict      predict         print       \n",
       "19  tensorflow  length     train       performance  binary          plot        \n",
       "\n",
       "    Topic # 07   Topic # 08   Topic # 09   Topic # 10  Topic # 11  \\\n",
       "0   words       features     time         na           clustering   \n",
       "1   word        feature      series       lt           cluster      \n",
       "2   text        selection    lstm         missing      distance     \n",
       "3   word2vec    pca          day          nan          clusters     \n",
       "4   document    importance   predict      inferred     similarity   \n",
       "5   documents   extraction   rnn          id           matrix       \n",
       "6   sentence    correlation  prediction   values       means        \n",
       "7   vectors     categorical  days         error        cosine       \n",
       "8   vector      vector       date         frame        points       \n",
       "9   topic       importances  sequence     auto         algorithm    \n",
       "10  embedding   dataset      model        jan          euclidean    \n",
       "11  corpus      classifier   event        feb          two          \n",
       "12  lda         one          month        mar          kmeans       \n",
       "13  topics      important    arima        function     number       \n",
       "14  embeddings  correlated   anomaly      matrix       method       \n",
       "15  similarity  different    one          correlation  use          \n",
       "16  sentences   xgboost      forecast     numeric      similar      \n",
       "17  context     space        forecasting  mean         metric       \n",
       "18  doc2vec     engineering  would        factor       find         \n",
       "19  idf         use          signal       rating       graph        \n",
       "\n",
       "       Topic # 12 Topic # 13  Topic # 14      Topic # 15  \n",
       "0   network        data       partial     image           \n",
       "1   neural         science    frac        images          \n",
       "2   layer          set        theta       class           \n",
       "3   output         orange     amp         cnn             \n",
       "4   input          analysis   function    classes         \n",
       "5   layers         big        sigma       dataset         \n",
       "6   weights        like       sum         object          \n",
       "7   networks       file       cost        classification  \n",
       "8   hidden         new        error       label           \n",
       "9   convolutional  would      gradient    detection       \n",
       "10  neurons        missing    hat         labels          \n",
       "11  net            pca        mathbf      imagenet        \n",
       "12  function       table      log         convolutional   \n",
       "13  number         different  loss        one             \n",
       "14  training       lt         mu          trained         \n",
       "15  activation     good       alpha       bounding        \n",
       "16  convolution    mining     equation    inception       \n",
       "17  neuron         sets       delta       resolution      \n",
       "18  cnn            want       bmatrix     recognition     \n",
       "19  inputs         training   derivative  pixel           "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nmf_topics(model_NMF, num_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
